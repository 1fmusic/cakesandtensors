{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm detection using a Convolutional Neural Net\n",
    "\n",
    "**Author:** Alexandra Ding\n",
    "\n",
    "**Date last modified:** 2/21/2018\n",
    "\n",
    "**Domain Generating Algorithms** (DGAs) are utilized by malware to generate domain names. Given a domain name such as 'google.com' or 'ejwfeijwofweofeofejfj833.net', this below model predicts whether this name was generated by a DGA. Examples are shown on how to train a deep learning model in Python Keras, as well as train and perform inference on multiple GPUs. As this notebook was last run on a p2.xlarge instance on Amazon Web Services- to experiment with the multi-GPU code, I recommend using a p2.8xlarge. \n",
    "\n",
    "Training datasets are the [Majestic Million](https://majestic.com/reports/majestic-million) and [Bambenek DGA](http://osint.bambenekconsulting.com/feeds/dga-feed.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.backend import manual_variable_initialization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Jupyter notebook is running on GPU\n",
    "device_name=\"/gpu:0\"\n",
    "shape=(int(10000),int(10000))\n",
    "\n",
    "with tf.device(device_name):\n",
    "    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n",
    "    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    sum_operation = tf.reduce_sum(dot_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net\n",
    "\n",
    "For this example, we will compile and train an open-source model from [Bin Yu et al. 2017](http://faculty.washington.edu/mdecock/papers/byu2017a.pdf) in Keras with a Tensorflow backend. These frameworks are commonly used for model prototyping, but have limitations (memory, speed) for model deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile Bin Yu et al. 2017's CNN Model\n",
    "\n",
    "# Specify Parameters\n",
    "max_len = 63 # Denotes the maximum length of a domain name\n",
    "input_tokens = 256 # Number of possible characters in an ASCII encoding\n",
    "embed_size = 128 # Dimensionality of embedding space\n",
    "\n",
    "def build_compile_model():\n",
    "    # Use Keras Sequential to add layers\n",
    "    model = Sequential(name='Seq')\n",
    "    model.add(Embedding(input_tokens, embed_size,\n",
    "                        input_length=max_len))\n",
    "    model.add(Conv1D(1000, 2, padding='same',\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu',\n",
    "                    kernel_initializer='glorot_normal'))\n",
    "    model.add(Dense(1, activation='sigmoid',\n",
    "                    kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Compile Model using Adam optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Print model layers and input/output dimensions\n",
    "    print(model.summary())\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def train_save_model(dga_model, X, y, n_epochs=20, \n",
    "                     filename='dga_model'):\n",
    "    \"\"\"\n",
    "    Train DGA deep learning model and save trained model several ways.\n",
    "    Args:\n",
    "        dga_model: compiled Keras model\n",
    "        X: features in training set\n",
    "        y: labels in training set\n",
    "        n_epochs: number of training epochs (default 20)\n",
    "        filename: what to save file as\n",
    "    Returns:\n",
    "        dga_model: trained Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    print(dga_model.summary())    \n",
    "    start = time.time()\n",
    "    history_fit = dga_model.fit(X, \n",
    "                          y,\n",
    "                          batch_size = 100,\n",
    "                          epochs = n_epochs,\n",
    "                          shuffle = True,\n",
    "                          verbose = 1,\n",
    "                          validation_split = 0.2)\n",
    "    \n",
    "    # Ensure that variables do not get reinitialized right after training\n",
    "    manual_variable_initialization(True)\n",
    "\n",
    "    # Save model as .h5 and .hdf5 (Keras default)\n",
    "    dga_model.save(filename+'.h5') \n",
    "    dga_model.save(filename+'.hdf5')\n",
    "    \n",
    "    print(\"Saved model\")\n",
    "    return dga_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Open-source) training datasets\n",
    "\n",
    "For this example, we pull open-source datasets containing known DGA domains ([Bambenek DGA Archive](https://osint.bambenekconsulting.com/feeds/dga-feed.txt)) and popular domains ([Majestic Millions](https://majestic.com/reports/majestic-million)). The Bambenek DGA Archive contains malicious names generated by more than 20 DGA families, as well as domains from several botnets, including Conficker, Kraken, Torpig, Kwyjibo. The Majestic Millions dataset contains the million domains with the top number of referring subnets- we assume that the majority of these domains are benign. Other approaches for generating datasets to train DGA detection models include taking historical network traffic and flagging domains which have never resolved, and/or have resulted in an NxDomain response. See the Yu et al. 2017 publication for further details. \n",
    "\n",
    "To preprocess the domain name inputs, we convert strings to lowercase and zero-padded the strings up to the max length for a domain name. They are then ASCII encoded (i.e. strings are mapped to ints). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def vectorize_domain(domain, max_len):\n",
    "    \"\"\"\n",
    "    Convert a single domain name to a 1d array of integers (ASCII encoding).\n",
    "\n",
    "    Args:\n",
    "        domain: the domain name to be vectorized (ex: 'goog.com')\n",
    "        max_len: maximum domain length (Default 110)\n",
    "    Returns:\n",
    "        Numpy array (dim max_len x 1) containing vectorized name\n",
    "    \"\"\"\n",
    "    data = np.zeros((max_len), dtype=np.int32)\n",
    "    for index in range(0, len(domain)):\n",
    "        if index < max_len:\n",
    "            data[index] = ord(domain[index])\n",
    "    return np.array(data, dtype=np.int32, ndmin=2)\n",
    "\n",
    "def column_vectorizer(domain_array, max_len=110):\n",
    "    \"\"\"\n",
    "    Helper function takes a numpy array column or pandas series values\n",
    "    containing domain names, vectorizes the names, and outputs a numpy array.\n",
    "\n",
    "    Args:\n",
    "        domain_series\n",
    "        max_len: Max length of domain (Default 110)\n",
    "\n",
    "    Returns:\n",
    "        array containing vecotrized domain names\n",
    "        (dim: n_domains x max_len)\n",
    "    \"\"\"\n",
    "    domain_data = np.zeros((len(domain_array), max_len), dtype=np.int32)\n",
    "\n",
    "    for i in range(len(domain_array)):\n",
    "        domain_data[i] = vectorize_domain(domain_array[i], max_len)\n",
    "\n",
    "    return np.array(domain_data, dtype=np.int32)\n",
    "\n",
    "def stratified_split_dga_benign(benign_domains_raw,\n",
    "                                dga_domains_raw,\n",
    "                                prop=0.2,\n",
    "                                rs=42):\n",
    "    \"\"\"\n",
    "    Split DGAs into Train and Test sets by family, and split the benign.\n",
    "\n",
    "    Args:\n",
    "        benign_domains_raw: list of benign domains\n",
    "        dga_domains_raw: list of dga domains\n",
    "        prop: proportion to use as test\n",
    "        rs: random seed\n",
    "\n",
    "    Returns:\n",
    "        X_train: domain names in training set\n",
    "        X_test: domain names in testing set\n",
    "        y_train: labels in training set\n",
    "        y_test: labels in testing set\n",
    "\n",
    "    \"\"\"\n",
    "    X_dga = dga_domains_raw['Domain'].values\n",
    "    y_dga = dga_domains_raw['Family'].values\n",
    "\n",
    "    X_benign = benign_domains_raw.values.flatten()\n",
    "    y_benign = np.tile('benign', (X_benign.shape[0]))\n",
    "\n",
    "    X = np.concatenate((X_dga, X_benign))\n",
    "    y = np.concatenate((y_dga, y_benign))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=prop,\n",
    "                                                        random_state=rs)\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized google.com is: \n",
      " [[103 111 111 103 108 101  46  99 111 109   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Example of vectorized domain\n",
    "print(\"Vectorized google.com is: \\n\", vectorize_domain('google.com', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0             Domain Family\n",
      "0           0  isdpjdnnjhibq.com     cl\n",
      "1           1  vegmjnsqtwuka.net     cl\n",
      "2           2  jtfansoakdfga.biz     cl\n",
      "3           3   qsjiqfrcvnhch.ru     cl\n",
      "4           4  eiivuknlmtrxx.org     cl\n",
      "DGA Size: (957049, 3)\n",
      "          Domain\n",
      "0     google.com\n",
      "1   facebook.com\n",
      "2    youtube.com\n",
      "3    twitter.com\n",
      "4  microsoft.com\n",
      "Benign Size: (1000000, 1)\n",
      "['childhoodexpect.net' 'urbackup.org' 'bvohellefrictionlessv.com' ...\n",
      " 'owuekspqehcr.com' 'mntqnwesbssp.com' 'kkmngrvwtuxw.pw']\n",
      "['pizd' 'benign' 'banjori' ... 'tinba' 'ramnit' 'tinba']\n",
      "[1 0 1 ... 1 1 1]\n",
      "(1565639, 63)\n",
      "(391410, 63)\n"
     ]
    }
   ],
   "source": [
    "# load datasets- these are saved locally(you will have to pull them from the urls above)\n",
    "# Load bambenek DGA feeds\n",
    "bamb_df = pd.read_csv('~/DGA_data/merged_feeds.csv')\n",
    "print(bamb_df.head(n=5))\n",
    "print(\"DGA Size:\", bamb_df.shape)\n",
    "\n",
    "# Load majestic millions\n",
    "mm_df = pd.read_csv('~/DGA_data/majestic_million.csv')\n",
    "print(mm_df.head(n=5))\n",
    "print(\"Benign Size:\", mm_df.shape)\n",
    "\n",
    "# Stratified split into train/test datasets\n",
    "[X_train, X_test, y_train_multiclass, y_test_multiclass]= stratified_split_dga_benign(mm_df, bamb_df, prop=0.2)\n",
    "y_train = y_train_multiclass != 'benign'\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test_multiclass != 'benign'\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train_multiclass)\n",
    "print(y_train)\n",
    "\n",
    "# Vectorize the domain names\n",
    "X_train_vec = column_vectorizer(X_train, max_len)\n",
    "print(X_train_vec.shape)\n",
    "X_test_vec = column_vectorizer(X_test, max_len)\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save a CNN in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 63, 128)           32768     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 63, 1000)          257000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 63, 1000)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 63000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               6300100   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 6,589,969\n",
      "Trainable params: 6,589,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 63, 128)           32768     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 63, 1000)          257000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 63, 1000)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 63000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               6300100   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 6,589,969\n",
      "Trainable params: 6,589,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1252511 samples, validate on 313128 samples\n",
      "Epoch 1/20\n",
      "1252511/1252511 [==============================] - 414s 330us/step - loss: 0.0570 - acc: 0.9797 - val_loss: 0.0430 - val_acc: 0.9847\n",
      "Epoch 2/20\n",
      "1252511/1252511 [==============================] - 414s 330us/step - loss: 0.0398 - acc: 0.9859 - val_loss: 0.0377 - val_acc: 0.9871\n",
      "Epoch 3/20\n",
      "1252511/1252511 [==============================] - 414s 331us/step - loss: 0.0351 - acc: 0.9876 - val_loss: 0.0331 - val_acc: 0.9885\n",
      "Epoch 4/20\n",
      "1252511/1252511 [==============================] - 413s 330us/step - loss: 0.0320 - acc: 0.9887 - val_loss: 0.0325 - val_acc: 0.9891\n",
      "Epoch 5/20\n",
      "1252511/1252511 [==============================] - 415s 332us/step - loss: 0.0298 - acc: 0.9896 - val_loss: 0.0319 - val_acc: 0.9892\n",
      "Epoch 6/20\n",
      "1252511/1252511 [==============================] - 414s 331us/step - loss: 0.0286 - acc: 0.9899 - val_loss: 0.0311 - val_acc: 0.9896\n",
      "Epoch 7/20\n",
      " 489300/1252511 [==========>...................] - ETA: 3:58 - loss: 0.0263 - acc: 0.9909"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2deec21cb36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdga_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_compile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_save_model(dga_model, X_train_vec, y_train, n_epochs=20, \n\u001b[0;32m----> 3\u001b[0;31m                      filename='model')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-647f75de2e19>\u001b[0m in \u001b[0;36mtrain_save_model\u001b[0;34m(dga_model, X, y, n_epochs, filename)\u001b[0m\n\u001b[1;32m     52\u001b[0m                           \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                           \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                           validation_split = 0.2)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Ensure that variables do not get reinitialized right after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dga_model = build_compile_model()\n",
    "train_save_model(dga_model, X_train_vec, y_train, n_epochs=20, \n",
    "                     filename='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on multiple GPUs in Keras\n",
    "\n",
    "Code is pulled from Keras documentation and from an online [demo](https://www.pyimagesearch.com/2017/10/30/how-to-multi-gpu-training-with-keras-python-and-deep-learning/). Keras multi_gpu_model uses data parallelism to partition the workload over multiple devices. Assume there are n devices. Then each one will receive a copy of the complete model and train it on 1/n of the data. The results such as gradients and updated model are communicated across these devices. Data parallelism is also possible in Gluon, MxNet and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "NUM_GPUS = 8\n",
    "\n",
    "dga_model = build_compile_model()\n",
    "\n",
    "# Replicates `model` on 8 GPUs.\n",
    "# This assumes that your machine has 8 available GPUs.\n",
    "parallel_model = multi_gpu_model(dga_model, gpus=NUM_GPUS)\n",
    "parallel_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam')\n",
    "\n",
    "# This `fit` call will be distributed on 8 GPUs.\n",
    "# Since the batch size is 256, each GPU will process 32 samples.\n",
    "parallel_model.fit(x, y, epochs=20, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on multi-GPUs\n",
    "\n",
    "We can load a copy of a saved Keras model to multiple GPUs, and again use **data parallelism** to deliver a subset of each data batch to each GPU for inference. This example uses Python Multiprocessing to initialize the workers on each GPU. In a deployment environment, data is coming off of an [Apache Kafka](https://kafka.apache.org/) topic, which has its own API and controls batch size. The below example should work with a Pandas dataframe or Numpy array as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Process,Manager,set_start_method\n",
    "\n",
    "def worker(batch, gpuid, d, filename='saved_model.h5'):\n",
    "    \"\"\"Worker function.\n",
    "\n",
    "    Sets the number of visible cuda devices and executes the main function\n",
    "    for a deep learning model.\n",
    "\n",
    "    Args:\n",
    "        batch: batch size\n",
    "        gpuid: the GPU that will be utilized by the processes\n",
    "        d: the shared dictionary\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpuid)\n",
    "\n",
    "    df = pd.DataFrame(batch).fillna(value=\"none\")\n",
    "    model = load_model(filename)\n",
    "    domain_vec = column_vectorizer(df['query'].str[0],75)\n",
    "    df['pred']= model.predict(domain_vec)\n",
    "\n",
    "    split = np.array_split(df[['pred','flow_id']],5)\n",
    "\n",
    "    for batch in split:\n",
    "        result = batch.to_csv(index=False, header=False)\n",
    "        d['output'] = d['output'] + [result]\n",
    "    return\n",
    "\n",
    "\n",
    "def execute_inference_batching(data):\n",
    "    \"\"\"Execute main function.\"\"\"\n",
    "    # Split array into multiple sub-arrays\n",
    "    l = np.array_split(np.array(data),3)\n",
    "    workers = []\n",
    "    gid = 0\n",
    "\n",
    "    for batch in l:\n",
    "        worker_assignment = Process(target=worker,\n",
    "                                    args=(batch.tolist(), gid, d))\n",
    "        workers.append(worker_assignment)\n",
    "        worker_assignment.start()\n",
    "        gid+=1\n",
    "\n",
    "    for w in workers:\n",
    "        w.join()\n",
    "\n",
    "    print(\"at producer for loop\")\n",
    "    for arr in d['output']:\n",
    "        print('length: ')\n",
    "        print(len(arr))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the function as\n",
    "# execute_inference_batching(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
